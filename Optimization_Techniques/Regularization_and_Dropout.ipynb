{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Regularization and Dropout.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dphi-official/Deep_Learning_Bootcamp/blob/master/Optimization_Techniques/Regularization_and_Dropout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAsen8s8_qQ0",
        "colab_type": "text"
      },
      "source": [
        "**Author:** [Hasan Ali](https://www.linkedin.com/in/learnmlwithhasan/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thhorARjsDPx",
        "colab_type": "text"
      },
      "source": [
        "### Regularization\n",
        "\n",
        "Regularization is a technique which makes slight modifications to the learning algorithm such that the model generalizes better. This in turn improves the modelâ€™s performance on the unseen data as well. \n",
        "\n",
        "Remember when we were adding more layers to the model (making it more complex) ? Adding more than required layers might also lead to overfitting. \n",
        "\n",
        "*By looking at the graph below can you guess, what should be an ideal model complexity.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKSiPd8lsDPy",
        "colab_type": "text"
      },
      "source": [
        "![perfect_point.png](https://drive.google.com/uc?export=view&id=1isDRyvRtMFPhC8RQHL9Puh-OoOm0z9FP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pP3DVnQKsDP7",
        "colab_type": "text"
      },
      "source": [
        "Somethings in Deep Learning can't be learnt in the first go. Regularisation is one of them. It takes some time. So, dont be hard on yourself and let this absorb.\n",
        "\n",
        "Here is how to use it in your neural networks.\n",
        "```python\n",
        "model.add(Dense(256,activation='relu', kernel_regularizer = 'l2'))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Trp7cAh_sDP8",
        "colab_type": "text"
      },
      "source": [
        "### Dropouts\n",
        "\n",
        "Dropout is a regularization method that approximates training a large number of neural networks with different architectures in parallel. As mentioned, it is preferrably used when training a large neural network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVgWUCb4sDP9",
        "colab_type": "code",
        "colab": {},
        "outputId": "a6c42f40-1f74-47d2-ebc9-190c46af9f04"
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(url=\"https://miro.medium.com/max/1200/1*iWQzxhVlvadk6VAJjsgXgg.png\", width=800, height=500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://miro.medium.com/max/1200/1*iWQzxhVlvadk6VAJjsgXgg.png\" width=\"800\" height=\"500\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbaNQNWysDQB",
        "colab_type": "text"
      },
      "source": [
        "*By dropping a unit out, we mean temporarily removing it from the network, along with all its incoming and outgoing connections*\n",
        "\n",
        "Dropout has the effect of making the training process noisy, forcing nodes within a layer to probabilistically take on more or less responsibility for the inputs.\n",
        "This conceptualization suggests that perhaps dropout breaks-up situations where network layers co-adapt to correct mistakes from prior layers, in turn making the model more robust."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21l5k4oosDQC",
        "colab_type": "text"
      },
      "source": [
        "It can be used with most types of layers, such as dense fully connected layers, convolutional layers, and recurrent layers such as the long short-term memory network layer. Dropout may be implemented on any or all hidden layers in the network as well as the visible or input layer. \n",
        "\n",
        "`NOTE` : It is not used on the output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68kgsPj1sDQK",
        "colab_type": "code",
        "colab": {},
        "outputId": "31512678-277c-4e9e-c9d6-ccdf06f171e4"
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "# import matplotlib for visualization\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\pele1\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctJo_QQrsDQO",
        "colab_type": "code",
        "colab": {},
        "outputId": "562172c0-23f5-465f-93fd-d750676d6594"
      },
      "source": [
        "images = X_train[:3]\n",
        "labels = y_train[:3]\n",
        "\n",
        "f,ax = plt.subplots(nrows=1,ncols=3, figsize=(20,4))\n",
        "\n",
        "for index,(img, ax) in enumerate(zip(images, ax)):\n",
        "    \n",
        "    ax.imshow(img,cmap='gray')\n",
        "    ax.axis('off')\n",
        "    ax.text(0.6,-2.0, f\"Digit in the Image {labels[index]}\", size=15, ha=\"center\")\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKSBryiPsDQS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import regularizers\n",
        "from datetime import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9qUbXZIsDQZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###why did we reshaped ?? Question for you !\n",
        "\n",
        "X_train = X_train.reshape(60000, 784)\n",
        "X_test = X_test.reshape(10000, 784)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjlAMNCXsDQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###what is to_categorical doing here ? Question for you !\n",
        "\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4afU-2YsDQg",
        "colab_type": "code",
        "colab": {},
        "outputId": "62d2978b-e183-41ae-cea5-911d3dc331fc"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(10, activation='relu', input_shape= (784,) ))\n",
        "model.add(Dropout(0.2)) ###using dropout\n",
        "model.add(Dense(256,activation='relu', kernel_regularizer = 'l2'))###using regularizer\n",
        "model.add(Dropout(0.2))###using dropout\n",
        "model.add(Dense(256,activation='relu', kernel_regularizer = 'l2')) ###using regularizer\n",
        "model.add(Dropout(0.2))###using dropout\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.001), metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\pele1\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pWxY6lisDQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logdir = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = TensorBoard(log_dir=logdir)\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrW9m1nJsDQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8rhtyNmsDQq",
        "colab_type": "code",
        "colab": {},
        "outputId": "1f8b45ad-c108-42e1-a842-71709dcc1ca7"
      },
      "source": [
        "training_history = model.fit(\n",
        "    X_train, # input\n",
        "    y_train, # output\n",
        "    batch_size=32,\n",
        "    verbose=1, # Suppress chatty output; use Tensorboard instead\n",
        "    epochs=10,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[tensorboard_callback, es],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "WARNING:tensorflow:From C:\\Users\\pele1\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 10s 166us/sample - loss: 4.7879 - acc: 0.2098 - val_loss: 4.4647 - val_acc: 0.2782\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 9s 153us/sample - loss: 4.4392 - acc: 0.2465 - val_loss: 4.2363 - val_acc: 0.2825\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 9s 153us/sample - loss: 4.2254 - acc: 0.2542 - val_loss: 4.0222 - val_acc: 0.2955\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 9s 154us/sample - loss: 4.0406 - acc: 0.2549 - val_loss: 3.8429 - val_acc: 0.3008\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 9s 154us/sample - loss: 3.8716 - acc: 0.2603 - val_loss: 3.6720 - val_acc: 0.3056\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 9s 155us/sample - loss: 3.7136 - acc: 0.2666 - val_loss: 3.5151 - val_acc: 0.3159\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 9s 155us/sample - loss: 3.5542 - acc: 0.2764 - val_loss: 3.3222 - val_acc: 0.3452\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 9s 155us/sample - loss: 3.3849 - acc: 0.2868 - val_loss: 3.1724 - val_acc: 0.3601\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 9s 158us/sample - loss: 3.2336 - acc: 0.2999 - val_loss: 2.9951 - val_acc: 0.3883\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 9s 158us/sample - loss: 3.0061 - acc: 0.3553 - val_loss: 2.6455 - val_acc: 0.5065\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8IUEJ1ysDQu",
        "colab_type": "text"
      },
      "source": [
        "This is just for demo purpose, dropouts is majorly used in deep/wide neural networks. This is a very basic one. As you can see, the accuracy is increasing very slowly and steadily and the model is yet to converge it will take around 100 more epochs. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKxc6qS9sDQv",
        "colab_type": "text"
      },
      "source": [
        "#### Why not just use early stopping rather than regularisation ?\n",
        "\n",
        "The main downside of early stopping is that this couples two tasks:\n",
        "\n",
        "1. Algorithm to optimize the cost function j (eg gradient descent, adam etc)\n",
        "2. Prevent overfitting (ie get more data, regularization)\n",
        "\n",
        "because by stopping gradient decent early, we are sort of breaking whatever we are doing to optimize cost function J and simultaneously trying to not over fit. Rather than using early stopping, one alternative is just use L2 regularization then we can just train the neural network as long as possible(the downside here is we have to try a lot of values of the regularization parameter lambda and hence it becomes computationally expensive)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVI257GmsDQw",
        "colab_type": "text"
      },
      "source": [
        "**Points to conclude with**\n",
        "\n",
        "- Large weights in a neural network are a sign of a more complex network that has overfit the training data.\n",
        "- Probabilistically dropping out nodes in the network is a simple and effective regularization method.\n",
        "- A large network with more training and the use of a weight constraint are suggested when using dropout."
      ]
    }
  ]
}
